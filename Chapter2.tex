    %%%%%%%%%%%%%
    We now extend the definition of the information measures beyond discrete distributions to more general spaces. First, we consider the case of continuous distributions in $\mathbb{R}^d$, and introduce the analogs of entropy, relative entropy, and mutual information. We study their properties and compare them with their discrete counterparts. Next, we define relative entropy and mutual information for general probability spaces, and establish two variational definitions. Finally, we introduce a class of information measures that generalize relative entropy, called the $f$-divergences, and study some of their properties. 
    %%%%%%%%%%%%%

 
    \section{Continuous Distributions}
        First we focus on the case of continuous distributions on $\X =\reals^d$. That is, we focus on distributions $P_X$ which admit a density, denoted by $f_X$, with respect to the Lebesgue measure on $\X$. This implies that for any measurable $E \subset \X$, we have $P_X = \int_{\X} \boldsymbol{1}_E(x) f_X(x) dx  = \int_{E} f_X(x) dx$. For distributions $(X, Y)$ with joint density $f_{XY}$, we also assume the existence of conditional densities $f_{Y|X}$ and $f_{X|Y}$ satisfying 
        \begin{align}
            P_{XY}(E) = \int_{\X}f_X(x)dx \int_{\Y} f_{Y|X}(y) \boldsymbol{1}_E(x,y)dy = \int_{\Y}f_Y(y)dy \int_{\X} f_{X|Y}(x) \boldsymbol{1}_E(x,y)dx.  
        \end{align}
        For such distributions, we can define the continuous analogs of entropy, relative entropy, and mutual information, as well as their conditional variants. 

        \begin{definition}[Differential Entropy]
            \label{def:diff-entropy}
            The differential entropy of a continuous random variable $X$, with density $f_X$, is defined as 
            \begin{align}
                h(X) \equiv h(f_X) = -\int_{S} f_X(x) \log(f_X(x)) dx, 
            \end{align}
            where $S = \{x \in \X: f(x) > 0\}$ is the support of $f_X$. 
            Similarly, conditional entropy of $Y$ given $X$ is defined as follows (where the integrals are over the appropriate supports): 
            \begin{align}
                h(Y|X) = -\int f_X(x) dx \int  f_{Y|X}(y|x) \log \lp f_{Y|X}(y|x) \rp dy. 
            \end{align}
            As in the discrete case, we can write the joint differential entropy $h(X, Y)$ as 
            \begin{align}
                h(X, Y) = h(X) - h(Y|X) = h(Y) - h(Y|X).  \label{eq:joint-diff-entropy}
            \end{align}
        \end{definition}
        The definition of joint entropy in~\eqref{eq:joint-diff-entropy} implicitly assumes that at least of the terms is not infinite.  We now evaluate the differential entropy for some common distributions.
        \begin{example}[Uniform distribution]
            \label{example:diff-ent-uniform}
            Suppose $\X = [a, b]$, and $X$ is a the uniformly distributed random variable over $\X$ with density $f_X(x) = 1/(b-a)$ for all $x \in \X$. Then, the  differential entropy of $X$ is equal to 
            \begin{align}
                h(X) = \int_{\X} \frac{1}{b-a} \log(b-a) dx = \log(b-a). 
            \end{align}
            Thus, $h(X)=0$ for $(b-a)=1$, $h(X)<0$ for $(b-a)<1$, and $h(X)>0$ for $(b-a)>1$. This is in contrast with entropy defined for discrete distributions, which is always non-negative, and equal to $0$ only for Dirac distributions.  
        \end{example}

        \begin{example}[Multivariate Gaussian]
            \label{example:diff-ent-gaussian} 
            Consider a multivariate Gaussian random variable~($X$) over $\X = \reals^d$, with mean $\mu$ and covariance matrix $K$. The density of this random variable is 
            \begin{align}
                f_X(x) = \frac{1}{\sqrt{(2\pi)^n |K|}} \exp\lp - \frac{1}{2} (x-\mu)^TK^{-1}(x-\mu) \rp. 
            \end{align}
            Then, the differential entropy of $X$ is equal to 
            \begin{align}
                h(X)  &=  \log \lp \sqrt{(2\pi)^d} |K| \rp + \frac{1}{2}\int_{\X} (x-\mu)^{T} K^{-1} (x-\mu)f_X(x) dx\\
                &=  \frac{1}{2}\log \lp (2\pi)^d |K| \rp + \frac{1}{2} \mathbb{E}\lb (X-\mu)^{T} K^{-1} (X-\mu) \rb \\
                &= \frac{1}{2}\log \lp (2\pi)^d |K| \rp + \frac{1}{2} \sum_{i=1}^d \sum_{j=1}^d \mathbb{E}\lb (X_i - \mu_i)K^{-1}_{ij}(X_j-\mu_j)  \rb \\
                &= \frac{1}{2}\log \lp (2\pi)^d |K| \rp + \frac{1}{2} \sum_{i=1}^d \sum_{j=1}^d K^{-1}_{ij}\blue{\mathbb{E}\lb (X_i - \mu_i)(X_j-\mu_j)  \rb}  \\
                &= \frac{1}{2}\log \lp (2\pi)^d |K| \rp +  \frac{1}{2} \sum_{i=1}^d \sum_{j=1}^d K^{-1}_{ij}\blue{K_{ij}}  \\
                &= \frac{1}{2}\log \lp (2\pi)^d |K| \rp + \frac{1}{2} \sum_{i=1}^d \sum_{j=1}^d K^{-1}_{ij}K_{ji}  \\
                &=  \frac{1}{2}\log \lp (2\pi)^d |K| \rp + \frac{1}{2} \sum_{i=1}^d (I_d)_{ii} 
                = \frac{1}{2}\log \lp (2\pi)^d |K| \rp + \frac{d}{2}  \\
                & =  \frac{1}{2} \log \lp (2\pi e)^d |K| \rp \text{ nats/bits}. 
            \end{align}
            A simple consequence of this is that for univariate Gaussian with mean $\mu$, and variance $\sigma^2$, the differential entropy is equal to $(1/2) \log (2\pi e \sigma^2)$ nats/bits. 
        \end{example}

        \begin{remark}
            \label{remark:translation-invariant} Note that in both the examples considered above, the differential entropy is ``translation invariant'': that is, the differential entropy of $X$ and $X + c$ for any $c \in \X$ is the same. This property is true generally for differential entropy, beyond the two examples above, as we will see later. 
        \end{remark}

        We now introduce the definition of relative entropy for continuous distributions. 

        \begin{definition}
            \label{def:rel-ent-cont-1} Suppose $P_X$ and $P_Y$ are two continuous distributions on $\X$, with densities $f_X$ and $f_Y$ respectively. Then, the relative entropy between them is defined as~(with $S_X$ denoting the support of $P_X$): 
            \begin{align}
                \dkl(P_X \parallel P_Y) =
                \begin{cases}
                \int_{S_X} f_X(x) \log \lp \frac{f_X(x)}{f_Y(x)} \rp dx,  &\quad \text{if } \{f_Y=0\} \subset \{f_X=0\}, \\    
                 +\infty, &\quad \text{otherwise}. 
                \end{cases}
            \end{align}
            Similarly, let $P_{XY}$~(with density $f_{XY}$) and $Q_{XY}$~(with density $g_{XY}$) denote two continuous joint distributions over $\X \times \Y$. Then the conditional relative entropy between $P_{Y|X}$ and $Q_{Y|X}$ is defined as 
            \begin{align}
                \dkl(P_{Y|X} \parallel Q_{Y|X} | P_X) = \int f_X(x) dx \int f_{Y|X}(y|x) \log \lp \frac{f_{Y|X}(y|x)}{g_{Y|X}(y|x)} \rp dy. 
            \end{align}
        \end{definition}

        \begin{example}
            \label{example:rel-ent-cont-1} 
            The relative entropy between two univariate Gaussians, $P = N(\mu_1, \sigma_1^2)$ and $Q=N(\mu_2, \sigma_2^2)$ is equal to 
            \begin{align}
                \dkl(P \parallel Q) &= \mathbb{E}_P \lb \log (p(X)/q(X)) \rb = \mathbb{E}_P\lb \log\lp \frac{\sigma_2}{\sigma_1}\rp + \frac{1}{2}\lp \frac{(X-\mu_2)^2}{\sigma_2^2} - \frac{(X-\mu_1)^2}{\sigma_1^2} \rp \rb \\
                &= \log\lp \frac{\sigma_2}{\sigma_1}\rp + \frac{1}{2} \mathbb{E}_P\lb \lp \frac{(X-\mu_2)^2}{\sigma_2^2} - \frac{(X-\mu_1)^2}{\sigma_1^2} \rp \rb \\
                &= \log\lp \frac{\sigma_2}{\sigma_1}\rp + \frac{1}{2} \lp \frac{\sigma_2^2 + (\mu_1-\mu_2)^2}{\sigma_2^2}  - 1\rp 
                = \log\lp \frac{\sigma_2}{\sigma_1}\rp + \frac{(\mu_1-\mu_2)^2}{2}. 
            \end{align}
            Thus, if $\sigma_1 = \sigma_2$,  the relative entropy is proportional to the difference in mean squared. 
        \end{example}

        \begin{example}[Gaussian-Cauchy]
            Note that the relative entropy can be infinite even if the distributions are absolutely continuous. For example, consider the relative entropy between $P$ and $Q$, when $P$ has a Cauchy distribution with density $f(x) = 1/\lp \pi(x^2 + 1)\rp$, and $Q$ has a Gaussian distribution with density $g(x) = (1/\sqrt{2\pi})\exp(-x^2/2)$. Clearly, $P \ll Q$ and $Q\ll P$. However, 
            \begin{align}
                \dkl(P\parallel Q) = \mathbb{E}_P\lb  \log(p(X)/q(X) \rb \asymp -h(P) + \mathbb{E}_P[X^2] = \infty, 
            \end{align}
            since $h(P)$ can be shown to be finite, while the second moment of Cauchy distribution is infinite. 
        \end{example}

        Finally, we can now introduce the definition of mutual information for continuous random variables. 
        \begin{definition}
            \label{def:mi-cont}
            The mutual information between two continuous random variables $X$ and $Y$ is defined as 
            \begin{align}
                I(X;Y) = \int_{S_{XY}} f_{XY}(x, y) \log \lp \frac{ f_{XY}(x, y)}{f_X(x) f_Y(y)} \rp dx\, dy, 
            \end{align}
            where $S_{XY} = \{(x,y) \in \X \times \Y: f_X(x, y) > 0\}$ is the support of the joint density of $(X, Y)$. 
            It is easy to check that $I(X;Y) = h(X) - h(X|Y) = h(Y) - h(Y|X)$. 
        \end{definition}


        \subsection{Connections to discrete measures via quantization} 
            \label{subsec:info-measures-quantization} 

            \begin{example}
                Let $U$ denote the random variable with uniform distribution on $[0, a]$. For any $\Delta >0$, let $U_\Delta$ denote the discrete quantization of $U$: that is, $U_\Delta$ takes values in the set $\{i\Delta: 1 \leq i \leq a/\Delta\}$.  Then, we have the following relation: 
                \begin{align}
                    H(U_\Delta) = \log(a/\Delta) = \log (a) - \log(\Delta) = h(U) - \log(\Delta). 
                \end{align}
                Or in other words, for any $\Delta>0$~(such that $a/\Delta$ is an integer), we have 
                \begin{align}
                    H(U_\Delta) + \log(\Delta) = h(U).  
                \end{align}
            \end{example}
            We now show that the same relation between $H(\cdot)$ and $h(\cdot)$ holds more generally for continuous distributions. 

            \begin{definition}
                \label{def:quantized-rv-1}
                Let $X$ denote a real-valued random variable with continuous density $f_X$. For any $\Delta>0$, partition $\X =\reals$ into a (countable) grid of size $\Delta$, consisting of sets
                \begin{align}
                    E_i = [i\Delta, (i+1)\Delta), \quad \text{for } -\infty \leq i \leq \infty. 
                \end{align}
                Then, for any $i$, due to the mean-value theorem, there exists an $x_i \in E_i$, such that $f_X(x_i) \Delta = \int_{E_i} f_X(x) dx \defined p_i$. We define the $\Delta$-quantized version of $X$, denoted by $X_\Delta$, as 
                \begin{align}
                    X_\Delta = x_i, \quad \text{with probability } p_i, \quad \text{for } i \in \mathbb{Z}. 
                \end{align}
            \end{definition}

            \begin{theorem}
                \label{thm:quantized-entropy} Consider the case of $\X = \reals$, and let $X$ be a continuous distribution with (a Riemann integrable) density function $f_X$. 
                Then, we have the following: 
                \begin{align}
                    \lim_{\Delta \downarrow 0} H(X_\Delta) + \log(\Delta) = h(X) = h(f_X), 
                \end{align}
                where $X_\Delta$ is the $\Delta$-quantized version of $X$~(\Cref{def:quantized-rv-1}). 
            \end{theorem}
            \begin{proof}
                The proof follows directly from the definition of discrete entropy. In particular, 
                \begin{align}
                    -H(X_\Delta) &= \sum_{i \in \mathbb{Z}} p_i \log p_i = \sum_{i\in \mathbb{Z}} f_X(x_i) \Delta \lp \log (f_X(x_i)) + \log(\Delta) \rp \\
                    & = \sum_{i\in \mathbb{Z}} f_X(x_i) \Delta \log (f_X(x_i)) + \log(\Delta)\sum_{i \in \mathbb{Z}} p_i\\
                    & =  \sum_{i\in \mathbb{Z}} f_X(x_i) \Delta \log (f_X(x_i))  + \log(\Delta), 
                \end{align}
                where the last equality simply uses the fact that $\sum_{i \in \mathbb{Z}} p_i = 1$. On rearranging the above relation, we get 
                \begin{align}
                    H(X_\Delta) + \log \Delta =  -\sum_{i\in \mathbb{Z}} f_X(x_i) \Delta \log (f_X(x_i)).   
                \end{align}
                Since the term on the right is the Riemann sum for the integral $-\int_{\reals} f_X(x) \log(f_X(x)) dx$ that defines the differential entropy of $X$, we get the required result by taking the limit $\Delta \downarrow 0$: 
                \begin{align}
                     \lim_{\Delta \downarrow 0} H(X_\Delta) + \log \Delta =  h(f_X). 
                \end{align}
            \end{proof}

            \begin{remark}
                \label{remark:quantized-entropy-1}
                One interpretation of this above result is in terms of the number of bits needed to learn a $\Delta$-approximate version of a continuous random variable $X$. That is, to approximate a continuous random variable $X$, with an approximation error $\Delta$, we roughly need $\log(1/\Delta) + h(f_X)$ bits. For example, going back to our uniform example, to represent a uniform $[0,a]$ random variable with up to $\Delta$ error, we need $\log(a/\Delta)$ bits (exactly). 
            \end{remark}

            \begin{remark}
                \label{remark:quantized-entropy-2} Another interpretation of the above result is via the AEP for the discrete and continuous entropies. For concreteness, let $X$ be a uniformly distributed over $\X=[0,a]$ random variable, and $X_\Delta$ is its $\Delta$-quantization. Then, we have the following two statements: 
                \begin{itemize}
                    \item AEP for the differential entropy says that (with probability almost $1$), the $n$ \iid realizations of $X$ are concentrated in a volume of $\approx 2^{n h(X)}$ within the space $\X^n$. 
                    \item AEP for discrete entropy says that (with probability almost $1$) there are $\approx 2^{h H(X_\Delta)}$ equiprobable sequences of length $n$ in the quantized space $\X_\Delta^n$. 
                \end{itemize}
                Each point in $\X_\Delta^n$ denotes  a unique cube in $\X^n$, of volume $\Delta^n$. Hence, the total volume covered by the $2^{n H(X_\Delta)}$ sequences is $\approx \Delta^n 2^{n H(X_\Delta)}$. The result above says that the two volumes are approximately equal: that is, 
                \begin{align}
                    \Delta^n 2^{n H(X_\Delta)} \approx 2^{n h(X)}, \quad \text{ or } \quad  \log (\Delta) + H(X_\Delta) \approx h(X). 
                \end{align}
            \end{remark}

            Unlike the entropy, the behavior of relative entropy and mutual information is stable under quantization. In fact, through informal arguments, we can see that the continuous relative entropy (and thus mutual information as well) can be seen as the limits of their quantized versions, as $\Delta \to 0$. 

        \subsection{Properties}
        \label{subsec:continuous-properties}
            We begin by noting the nonnegativity of relative entropy for continuous distributions. 
            \begin{proposition}
                \label{prop:continuous-property-1} 
                For two $\X=\reals^d$-valued distributions $P$ and $Q$ with densities $f$ and $g$ respectively, we have 
                \begin{align}
                    \dkl(P \parallel Q) \equiv \dkl(f \parallel g) \geq 0, 
                \end{align}
                with equality if and only if $f=g$ almost everywhere. As consequence, we also have for any two continuous random variables $X$ and $Y$: 
                \begin{itemize}
                    \item $I(X; Y) \geq 0$, with equality if and only if $X \perp Y$. 
                    \item $h(X|Y) \leq h(X)$, with equality if and only if $X \perp Y$. 
                \end{itemize}
            \end{proposition}
            \begin{proof}
                The proof of the nonnegativity of $\dkl$ follows directly from the convexity of the mapping $\varphi(x) = x\log x$. First, note that if $P \not \ll Q$, then $\dkl(P\parallel Q) = \infty$, and there is nothing to prove. Hence, we can assume that $P \ll Q$, and $S_f = \{x:f(x)>0\} \subset S_g = \{x: g(x) > 0\}$. 
                Then, we have 
                \begin{align}
                    \dkl(f \parallel g) &= \int_{S_f} g(x) \varphi\lp \frac{f(x)}{g(x)} \rp dx  = \inf_{S_g} g(x) \varphi\lp \frac{f(x)}{g(x)} \rp dx   \\
                    & \geq \varphi \lp \int_{S_g} g(x) \frac{f(x)}{g(x)}dx \rp = \varphi(1) = 0. 
                \end{align}
                Since the $\geq$ above follows from Jensen's inequality, it holds with equality if and only if $f(x)/g(x)$ is a constant almost surely. Since both of them are densities, the constant must be $1$ and thus the equality holds when $f=g$ almost surely. 
            \end{proof}

            We now look at some properties of differential entropy. 
            \begin{proposition}
                \label{prop:differential-entropy} 
                The differential entropy of continuous random variables satisfies the following properties: 
                \begin{enumerate}[label=(\alph*)]
                    \item Chain rule: $h(X_1, \ldots, X_n) = \sum_{i=1}^n h(X_i|X^{i-1})$. 
                    \item Translation invariance: $h(X+c) = h(X)$ for all $c \in \X$. 
                    \item Effect of scaling: if $\X = \reals$, then $h(aX) = h(X) + \log(|a|)$. More generally, for vector valued $X$, we have $h(AX) = h(X) + \log(|\text{det}(A)|)$, where $A$ is an invertible matrix. 
                \end{enumerate}
            \end{proposition}

            \begin{proof}
                \begin{enumerate}[label=(\alph*)]
                    \item Follows directly from the definition.  
                    \item Let $Y = X+c$. Then, we have $f_Y(x) = f_X(x-c)$, and hence we have 
                    \begin{align}
                        h(Y) = \int_{c+S_X} f_X(x-c) \log(1/f_X(x-c)) dx = \int_{S_X} f_X(x) \log(1/f_X(fX)) dx = h(X). 
                    \end{align}
                    \item We prove the general multivariate case. Let $Y = AX$, and by the change of variable formula, we have 
                    \begin{align}
                        g(y) = \frac{1}{|A|} f(A^{-1}y).  
                    \end{align}
                    Then, we have the following 
                    \begin{align}
                        h(AX)= h(Y) &= -\int  g(y) \log (g(y)) = - \frac{1}{|A|} \int f(A^{-1}y) \lp \log(f(A^{-1}y) -\log(|A|) \rp  \\
                        & = \log(|A|) - \frac{1}{|A|} \int f(A^{-1}y) \log \lp f(A^{-1}y) \rp dy \\
                        & \stackrel{(i)}{=} \log(|A|) -  \int f(x) \log \lp f(x) \rp dx = h(X) + \log(|A|). 
                    \end{align}
                    In the equality $(i)$ above, we used the change of variable $x = A^{-1}y$, which implies $dx = |\text{det}(A^{-1})|dy = (1/|A|) dy$. 
                \end{enumerate}
            \end{proof}
            % 
             \begin{remark}
                \label{remark:diff-entropy-3} 
                Note that despite the scaling operation being a bijection, the differential entropy of the resulting variable (i.e., $Y=AX$) is different from the original~($X$). This is unlike the discrete entropy which did not change under such operations.  
            \end{remark}   
            % 
            We end this section with a result about a characterization of Gaussian in terms of the maximum entropy achieving distribution among the class of all distributions with finite second moment. 
            \begin{proposition}
                \label{prop:max-ent-1} Let $\mc{P}_2(\reals)$ denote the class of continuous probability distributions on $\reals$ with zero mean and with second moment upper bounded by $b<\infty$. Then, the distribution from $\mc{P}_2$ with the largest differential entropy is $N(0, b)$. 
            \end{proposition}
            \begin{proof}
                Without loss of generality, we assume that $b=1$. 
                
                Suppose $P$ be any distribution in $\mc{P}_2$, with density $f$, and Let $\phi$ denote the density of the standard normal random variable. Then, by the nonnegativity of relative entropy, we have 
                \begin{align}
                    0 & \leq \dkl(f \parallel \phi)  = \int_{\reals} f(x) \big(\log(f(x))  - \log(\phi(x)) \big) dx \\
                    & = -\int_{\reals} f(x) \log(\phi(x)) dx + \int_{\reals} f(x) \log(f(x)) dx \\
                    & = -\int_{\reals} \phi(x) \log(\phi(x)) dx - h(f) \\
                    & = h(\phi) - h(f). 
                \end{align}
                This completes the proof.
            \end{proof}

            \begin{example}[Capacity of Gaussian Channel] 
                \label{ex:gaussian-channel-capacity} Suppose $\X = \reals$, and consider a power-limited Gaussian channel, which takes in a $\X$-valued input $X$, and perturbs it with  an additive Gaussian noise $Z \sim N(0, \sigma_N^2)$ that is independent of $X$. The output of the channel is denoted by $Y= X+Z$. Suppose we have an energy constraint $\mathbb{E}[X^2] \leq \sigma_P^2$. Then, the capacity of this channel is equal to 
                \begin{align}
                    C = \sup_{f_X: \mathbb{E}[X^2] \leq \sigma_P^2} I(X; Y) = \frac{1}{2} \log \lp 1 + \frac{\sigma_P^2}{\sigma_N^2} \rp = \frac{1}{2} \log \lp 1 + \text{SNR} \rp. 
                \end{align}
                To see why this is true, not that for any arbitrary input $X$, we have 
                \begin{align}
                    I(X;Y) = h(Y) - h(Y|X) = h(Y) - h(X+Z|X) = h(Y) - h(Z|X) = h(Y) - h(Z), 
                \end{align}
                where the last equality uses the fact that $Z \perp X$. Hence, we have 
                \begin{align}
                    I(X;Y) = h(Y) - \frac{1}{2} \log (2\pi e \sigma_N^2) \leq \frac{1}{2} \log (2 \pi e \sigma_Y^2) - \frac{1}{2} \log (2 \pi e \sigma_N^2),
                \end{align}
                where the inequality uses~\Cref{prop:max-ent-1}. Since $X \perp X$, the variance of $Y$~(denoted by $\sigma_Y^2$) is equal to $\sigma_X^2 + \sigma_N^2$. Furthermore, under the constraint, the maximum value of $\sigma_X^2$ is equal to $\sigma_P^2$. Thus, we have 
                \begin{align}
                    I(X;Y) \leq \frac{1}{2} \log \lp \frac{\sigma_N^2 + \sigma_P^2}{\sigma_N^2} \rp = \frac{1}{2} \log \lp 1 + \frac{\sigma_P^2}{\sigma_N^2} \rp. 
                \end{align}
                Since $X$ was arbitrary, this also implies that $C \leq (1/2) \log(1 + \sigma_P^2/\sigma_N^2)$. Finally, note that the inequality holds with an equality for the input distribution $f_X = N(0, \sigma_P^2)$. 
            \end{example}
            
            In the next section, we illustrate some interesting consequences of the simple properties of continuous information measures we obtained above. 
            %%
        \subsection{Applications}
        \label{subsec:applications-continuous}
            We now look at some simple applications of the properties of continuous information measures. 

       \paragraph{Maximum Entropy distributions.} Staying with the theme of maximum entropy distributions, we show a general method for characterizing the distributions achieving the optimal entropy under different types of constraints. 
        
        Let $\mc{F}$ denote a class of densities on $\X = \reals$ satisfying $m$ integral constraints: 
        \begin{itemize}
            \item $f(x) \geq 0$, for all $x \in S$, with equality outside $S$. 
            \item $\int_S f(x) dx = 1$. 
            \item $\int_S f(x) r_i(x)dx = \alpha_i$, for $1 \leq i \leq m$. 
        \end{itemize}
        Our next result characterizes the distribution from $\mc{F}$ achieving the maximum entropy. 
        \begin{proposition}
            \label{prop:max-entropy} The density $f^* \in \mc{F}$ achieving the maximum entropy is of the form $f^*(x) = \exp \lp \lambda_0 + \sum_{i=1}^m \lambda_i r_i(x) \rp$ for all $x \in S$, for some real-valued terms $\{\lambda_i: 0 \leq i \leq m\}$, chosen to ensure $f^*$ satisfies the constraints.  
        \end{proposition}

        \begin{proof}
            We will use a slightly informal ``guess and verify'' strategy to prove this result. In particular, note that entropy is a concave functional of the densities, and the domain is convex. Hence, we can write the Lagrangian as 
            \begin{align}
                L(f) = - \int f \log f + \lambda_0 \int f + \sum_{i=1}^m \int f r_i.   
            \end{align}
            The proper way to proceed now is to take the functional derivative of $J$ with respect to $f$.  Instead, we present an informal argument. Suppose we write the integrals as approximate Riemann sums. That is, we consider a grid of width $\Delta$ partitioning the domain $\X$, and write 
            \begin{align}
                L(f) \approx - \sum_{j \in \mathbb{Z}} f(x_j) \log(f(x_j)) \Delta + \lambda_0 \sum_{i \in \mathbb{Z}} f(x_j) \Delta + \sum_{i=1}^m  \sum_{j \in \mathbb{Z}} f(x_j) r_i(x_j) \Delta. 
            \end{align}
            Now, we can consider $L(f)$ as a function of the variables $\{y_j = f(x_j): j \in \mathbb{Z} \}$. So taking the (usual) derivative of $L(f)$ with $y_j = f(x_j)$, and setting it to $0$,  we get 
            \begin{align}
                \frac{dL(f)}{dy_j} = \Delta \lp  - \log(f(x_j)) - 1 + \lambda_0 + \sum_{i=1}^m f(x_j) r_i(x_i)  \rp  = 0. 
            \end{align}
            The above equation suggests a solution of the form 
            \begin{align}
                f(x) = \exp \lp -1 + \lambda_0 + \sum_{i=1}^m \lambda_i r_i(x) \rp. 
            \end{align}
            Now, constraints give us $(m+1)$ equations in $(m+1)$ unknowns $(\lambda_i)_{i=0}^m$, and we can solve this linear system to construct a candidate solution $f^*$ of the above form, satisfying all the constraints. 


            Having informally justified the form of the candidate solution $f^*$, we now provide an information theoretic proof of its optimality. Let $g$ denote any element of $\mc{F}$. Then, we have 
            \begin{align}
                h(g) = - \int_{S} g \log g = - \int_S g \log(g/f^*) - \int_{S} g \log f^* = -\dkl(g\parallel f^*) - \int_S g\log f^*. 
            \end{align}
            Since $f^*$ is supported on the entire $S$, the above operation is valid. Due to the nonnegativity of relative entropy, the above result implies that 
            \begin{align}
                h(g) \leq - \int_S g \log f^*. 
            \end{align}
            If the integral on the right side were with respect to the density $f^*$, we would have obtained the required inequality. Nevertheless, we exploit the specific form of $f^*$ to show the required inequality: 
            \begin{align}
                -\int g \log f^* &= - \int g \lp \lambda_0 + \sum_{i=1}^m \lambda_i r_i \rp = -\lambda_0 \int_S g  - \sum_{i=1}^m \lambda_i \int_S g r_i \\
                & \stackrel{(i)}{=}  -\lambda_0 \int_S f^*  - \sum_{i=1}^m \lambda_i \int_S f^* r_i \\ 
                & = - \int_S f^* \log f^* = h(f^*). 
            \end{align}
             The equality $(i)$ follows from the fact that both $g$ and $f^*$ are elements of $\mc{F}$, and thus they satisfy the required constraints. 
        \end{proof}
        \begin{example}
            \label{example:max-entropy} The previous result can be used to characterize the max-entropy distribution in various settings. 
            \begin{itemize}
                \item $S=[a, b]$, and no other constraints. Then, the optimal distribution has the form $f^*(x) = e^{\lambda_0}$; that is, the max-ent distribution is the uniform distribution. 
                \item Suppose $S=\reals$, and the constraints are $\mathbb{E}[X] = 0$ and $\mathbb{E}[X^2]=\sigma^2$. Then, the optimal density has the form $f^*(x) = \exp \lp \lambda_0 + \lambda_1 x + \lambda_2 x^2 \rp$. It is easy to verify that this is a Gaussian distribution with mean $0$ and variance $\sigma^2$. 
                \item Suppose $S = [0, \infty)$, and the only constraint is $\mathbb{E}[X] = \mu$. Then, the optimal distribution is of the form $f^*(x) = \exp (\lambda_0 + \lambda_1 x)$, which turns out to be the exponential distribution with parameter $\mu$. That is, $f^*(x) = (1/\mu)e^{-x/\mu}$.  
                \item Suppose $S=[0, \infty)$, and the constraints are $\mathbb{E}[X]=\alpha_1$ and $\mathbb{E}[\log X] = \alpha_2$. Then, the optimal distribution has the form $f^*(x) = \exp \lp \lambda_0 + \lambda_1 x + \lambda_2 log(x) \rp = x^{\lambda_2} \exp \lp \lambda_0 + \lambda_1 x \rp$. This is the family of Gamma distributions.  

                \item For $S = \reals^d$, and the constraint that $E[X_i]=0$ for all $i\in [d]$, and $\mathbb{E}[X_i X_j] = K_{ij}$, where $K \equiv [K_{ij}]$ is a positive semi-definite matrix, a similar argument shows that the distribution achieving the maximum entropy is $N(0, K)$. 
            \end{itemize}
        \end{example}

            
            \paragraph{Determinant Inequalities.} The properties of continuous information measures, especially for Gaussian distributions, can be used to obtain elementary proofs of several nontrivial matrix inequalities. 
            \begin{proposition}
                The following statements are true. 
                \begin{enumerate}[label=(\alph*)]
                    \item  Suppose $K$ is an $n \times n$ positive semi-definite matrix. Then, we have 
                    \begin{align}
                        |K| \equiv |\text{det}(K)| \leq \prod_{i=1}^n K_{ii}, 
                    \end{align}
                    with equality if and only if $K$ is a diagonal matrix. 
                    \item  For any two psd matrices $K_1$ and $K_2$, and any $\lambda \in [0, 1]$, we have 
                    \begin{align}
                        |\lambda K_1 + \overline{\lambda} K_2| \geq |K_1|^{\lambda} |K_2|^{\overline{\lambda}}.   
                    \end{align}
                    Or, in other words, the log-determinant function is concave on the space of positive semi definite matrices. 
                \end{enumerate}
            \end{proposition}
            \begin{proof}
                \begin{enumerate}[label=(\alph*)]
                    \item Given the psd matrix $K$, consider the Gaussian random vector $(X_1, \ldots, X_n) \equiv X^n \sim N(0, K)$ on the domain $\X = \reals^n$. Then, the chain rule for differential entropy implies that 
                    \begin{align}
                        h(X^n) = \sum_{i=1}^n h(X_i|X^{i-1}) \leq \sum_{i=1}^n h(X_i). 
                    \end{align}
                    Since the marginals of $X^n$ are also univariate Gaussians, with $X_i \sim N(0, K_{ii})$, we observe that 
                    \begin{align}
                        \frac{1}{2} \log \lp (2\pi e)^n |K| \rp \leq \sum_{i=1}^n \frac{1}{2} \log \lp 2 \pi e |K_{ii}| \rp  = \frac{n}{2} \log(2\pi e) + \frac{1}{2}\log \lp \prod_{i=1}^n K_{ii} \rp.  
                    \end{align}
                    On cancelling $(n/2) \log(2\pi e)$ from both sides, and exponentiating, we get the required inequality. 

                    Furthermore, we know that the subadditivity of differential entropy holds with equality if and only if all the $X_i's$ are independent; or in other words, the matrix $K$ is a equal to $\text{diag}(K_{11}, \ldots, K_{nn})$. 

                    \item Let $X \sim N(0, K_1)$ and $Y \sim N(0, K_2)$ denote two independent multivariate Gaussian random variables on $\X = \reals^n$. For a fixed $\lambda \in [0,1]$, introduce a Bernoulli random variable $ \Lambda \sim \text{Bernoulli}(\lambda)$ independent of $X$ and $Y$, and use it to define the mixture $Z = \Lambda X + \overline{\Lambda}Y$. 

                    We first observe that the covariance of $Z$ is equal to $K_\lambda \defined \lambda K_1 + \overline{\lambda}K_2$. To see this, note that since $\mathbb{E}[Z]= 0$, we have 
                    \begin{align}
                        \text{Cov}(Z) = \mathbb{E}[ZZ^T] &= \mathbb{P}(\Lambda = 0) \mathbb{E}[YY^T|\Lambda=0] + \mathbb{P}(\Lambda = 1) \mathbb{E}[XX^T|\Lambda=1]  \\
                        &= \overline{\lambda} \mathbb{E}[YY^T] + \lambda \mathbb{E}[XX^T] \\
                        &= \overline{\lambda} K_2 + \lambda K_1 = K_\lambda.  \label{eq:ky-fan-1}
                    \end{align}

                    Now, we use the fact that conditioning reduces entropy, to observe 
                    \begin{align}
                        h(Z|\Lambda) \leq h(Z) \leq h\lp N(0, K_{\lambda}) \rp = \frac{1}{2} \log \lp (2\pi e)^n |K_\lambda| \rp. 
                    \end{align}
                    Finally, we calculate the conditional entropy of $Z$ given $\Lambda$: 
                    \begin{align}
                        h(Z|\Lambda) &= \lambda h(Z|\Lambda=1) + \overline{\lambda} h(Z|\lambda=0) \\
                        & = \lambda h(X) + \overline{\lambda} h(Y) \\
                        & = \frac{\lambda}{2} \log \lp (2 \pi e)^n |K_1| \rp  + \frac{\overline{\lambda}}{2} \log \lp (2 \pi e)^n |K_2| \rp. \label{eq:ky-fan-2}
                    \end{align}
                    Now, by combining~\eqref{eq:ky-fan-1} with~\eqref{eq:ky-fan-2}, we get the required statement that 
                    \begin{align}
                        \lambda \log(|K_1|)  + \overline{\lambda} \log(|K_2|) \leq \log(|K_\lambda|). 
                    \end{align}
                \end{enumerate}
            \end{proof}
            

        \paragraph{Estimation error.} For the case of discrete distributions, Fano's inequality gave us a lower bound on the probability of error in a hypothesis testing problem in terms of the entropy of the observations. We now present two analogous results for the problem of estimation. 

        \begin{proposition}
            \label{prop:continuous-estimation-error-1} 
            Consider the Markov chain $X \rightarrow Y \rightarrow \hatX$, where $\hatX$ is an estimate of the unknown random variable $X$. Then, we have 
            \begin{align}
                \mathbb{E}[(X-\hatX(Y))^2] \geq \frac{1}{2\pi e} 2^{h(X|Y)}. 
            \end{align}
            In the special case where the side information is not available, the above inequality reduces to 
            \begin{align}
                \mathbb{E}[(X- \hatX)^2] \geq \frac{1}{2 \pi e} 2^{h(X)}. 
            \end{align}
        \end{proposition}

        \begin{proof}
            To prove this result, we begin with the observation that 
            \begin{align}
                \mathbb{E}[(X - \hatX(Y))^2] &\geq \mathbb{E}[ (X - E[X|Y])^2 ] = \mathbb{E}_Y \lb \mathbb{E}_{X|Y} \lb (X - E[X|Y])^2 |Y\rb \rb  \\
                & =  \mathbb{E}_Y[\var(X|Y)].  \label{eq:estimation-1}
            \end{align}
        Now, observe that 
        \begin{align}
            h(X|Y) &= -\int_{\Y} f(y)dy  \int_{\X} f(x|y) \log (f(x|y)) dx \leq \int_{\Y} f(y) \frac{1}{2}\log \lp 2 \pi e \var(X|Y=y) \rp \\ 
            & \stackrel{(i)}{\leq} \frac{1}{2} \log \lp 2 \pi e \, \mathbb{E}_Y[\var(X|Y)] \rp, 
        \end{align}
        where $(i)$ follows from an application of  Jensen's inequality, along with the concavity of $x \mapsto \log(x)$. On rearranging the above inequality, we get 
        \begin{align}
            \frac{1}{2\pi e} 2^{2h(X|Y)} \leq \mathbb{E}_Y[\var(X|Y)]. \label{eq:estimation-2}
        \end{align}
        Together,~\eqref{eq:estimation-1} and~\eqref{eq:estimation-2} imply the required result. 
        \end{proof}


        \paragraph{Bounding the entropy of discrete distribution.} We end with one final application of~\Cref{prop:max-ent-1}. For the case of discrete distributions with finite support, we know that the entropy is upper bounded by $\log(|\X|)$. However, this bound is not applicable when the support of a discrete random variable is countable. We now show how to use the max entropy continuous distribution to obtain a non-vacuous upper bound on the entropy of  a discrete distribution with countable support. 

        \begin{proposition}
            \label{prop:diff-ent-bound-on-discrete}
            Let $\mc{X} = \{x_i: i \in \mathbb{N}\}$ denote a countable alphabet, and let $X$ be an $\mc{X}$-valued random variable with $\mathbb{P}(X=x_i)=p_i$ for $i \in \mathbb{N}$. Then, we have 
        \begin{align}
            H(X) = H(p_1, p_2, \ldots) &\leq \frac{1}{2} \log \lp (2\pi e) \times \lp \sum_{i=1}^{\infty} p_i i^2 - \lp \sum_{i=1}^{\infty} i p_i \rp^2 + \frac{1}{12} \rp \rp. 
        \end{align}
        \end{proposition}

        \begin{proof}
            Introduce a new random variable $Y$, such that $\mathbb{P}(Y=i) = p_i$. Note that $H(Y) = H(X)$, since the discrete entropy is independent of the actual values taken by the random variable. Now, let $U$ denote a Uniform~$([0,1])$ random variable, and use it to define $Z = Y + U$. 
            We can verify that $Z$ is a continuous random variable with a density $f_Z(x) = p_i$, where $i = \lfloor x \rfloor$. 

            Now, the discrete entropy of $Y$ is equal to 
            \begin{align}
                H(X) = H(Y) &= -\sum_{i \in \mathbb{N}} p_i \log p_i = \sum_{i \in \mathbb{N}} \int_{i}^{i+1} f_Z(x) \log \lp f_Z(x) \rp dx  \\
                & = -\int_{\reals} f_Z(x) \log(f_Z(x)) dx \\
                & = h(Z). 
            \end{align}
            Note that since $X$ and $U$ are independent, the variance of $Z$ is equal to $\var(Y) + \var(U)$. Thus the differential entropy of $h(Z)$ is upper bounded by that of a zero mean Gaussian with the same variance. Hence, we have 
            \begin{align}
                &H(X) = h(Z) \leq \frac{1}{2} \log \lp (2\pi e) \var(Z) \rp, \quad \text{where} \\
                & \var(Z) = \var(Y) + \var(U) = \sum_{i} i^2 p_i - \lp \sum_{i} i p_i \rp^2 + \frac{1}{12}. 
            \end{align}
        \end{proof}

        
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
    \section{General Distributions*}
        In this section, we present the general definition of relative entropy. To state the definition, we need to recall the Radon-Nikodym theorem. 
        \begin{fact}
            \label{fact:radon-nikodym}
            Consider a measurable space $(\X, \mc{F})$ with two $\sigma$-finite  measures $P$ and $Q$. Suppose that $P$ is absolutely continuous w.r.t. $Q$, denoted by $P \ll Q$, which means that $Q(E)=0 \Rightarrow P(E)=0$, for any $E \in \mc{F}$. Then, there exists a measurable function $f: \X \to [0, \infty)$, such that 
            \begin{align}
                P(E) = \int_{E} f dQ, \quad \text{for all } E \in \mc{F}. 
            \end{align}
            The (not necessarily unique) function $f$ is called the Radon-Nikodym derivative of $P$ w.r.t. $Q$, and is also denoted as $\frac{dP}{dQ}$.  
            If $P \ll Q$ and $Q \ll P$, we have 
            \begin{align}
                \frac{dP}{dQ} = \lp \frac{dQ}{dP} \rp^{-1}. 
            \end{align}
            Finally, if $P \ll Q$ and $Q \ll R$, then we have 
            \begin{align}
                \frac{dP}{dR} = \frac{dP}{dQ} \times \frac{dQ}{dR}. 
            \end{align}
        \end{fact}

        Since we deal with probability measures, that are finite and hence $\sigma$-finite, the only condition required for the existence of the Radon-Nikodym derivative is the absolute continuity. Using this, we have the following general definition of relative entropy. 
        \begin{definition}
            \label{def:rel-ent-general} 
            For any two distributions $P$ and $Q$, defined on a common measurable space $(\X, \mc{F})$, the relative entropy between $P$ and $Q$ is defined as 
            \begin{align}
                \dkl(P \parallel Q) \defined 
                \begin{cases}
                 \mathbb{E}_Q \lb \frac{dP}{dQ}(X) \log \frac{dP}{dQ}(X) \rb, & \quad \text{if } P\ll Q, \\
                 + \infty, & \quad \text{if } P \not \ll Q.  
                \end{cases}
            \end{align}
        \end{definition}

        \begin{fact}
            \label{fact:rel-ent-general-2}
            An equivalent definition of the relative entropy between $P$ and $Q$, is 
            \begin{align}
                \dkl(P \parallel Q) \defined 
                \begin{cases}
                 \mathbb{E}_P \lb \log \frac{dP}{dQ}(X) \rb, & \quad \text{if } P\ll Q, \\
                 + \infty, & \quad \text{if } P \not \ll Q.  
                \end{cases}
            \end{align}
            See Lemma 2.4 of Polyanskiy and Wu for a proof. 
        \end{fact}
        \begin{definition}
            \label{def:markov-kernel} Suppose $(\X, \F)$ and $(\Y, \G)$ denote two measurable spaces. Then, a Markov kernel from $\X$ to $\Y$ is a mapping $K:\G \times \X \to [0,1]$, such that 
            \begin{itemize}
                \item For every $x \in \X$, the mapping $K(\cdot, x): \G \to [0,1]$ is a probability measure on $(\Y, \G)$, 
                \item For every $E \in \G$, the mapping $K(E, \cdot): \X \to [0,1]$ is $(\F, \mathbb{B}_{[0,1]})$-measurable. 
            \end{itemize}
        \end{definition}        

        \begin{remark}
            The Markov kernel can be interpreted as a random mapping form $\X$ to a probability measure on $(\Y, \G)$. Hence, we will often denote it with $P_{Y|X}$ --- that is, it defines a probability distribution of $Y$ for every realization of $X$.  For the special case of finite $\X$ and $\Y$,  the Markov kernel $K \equiv P_{Y|X}$ is simply the  transition probability matrix of size $|\X|\times |\Y|$. 
        \end{remark}

        \begin{fact}[Disintegration Theorem]
            \label{fact:disintegration-theorem}
            Suppose $P_{XY}$ is a joint distribution on $\X \times \Y$, and $\Y$ is standard Borel~(complete separable metric space with the Borel sigma-algebra). Then, there exists a Markov kernel $K$, such that for any measurable $E \subset \X \times \Y$, we have 
            \begin{align}
                P_{XY}(E) = \int_{\X}P_X(dx) K(E^x|x), \quad \text{for } E^x \defined \{y \in \Y: (x,y) \in E_x\}. 
            \end{align}
        \end{fact}

        We can now define the general version of conditional relative entropy, using the Markov kernel. 
        \begin{definition}
            \label{def:cond-rel-ent-general} Suppose $X$ is an $\X$-valued, and $Y$ is a $\Y$-valued random variable. Let $P_{XY}$ and $Q_{XY}$ denote two joint distributions of $(X, Y)$. Then, if $\Y$ is standard Borel (or `nice' in the language used by Durrett), then the conditional relative entropy between $P_{Y|X}$ and $Q_{Y|X}$ given $P_X$ is defined as 
            \begin{align}
                \dkl(P_{Y|X} \parallel Q_{Y|X} | P_X) = \mathbb{E}_{P_X}\lb \dkl\lp P_{Y|X}(\cdot, X) \parallel Q_{Y|X}(\cdot, X) \rp \rb. 
            \end{align}
        \end{definition}

        \begin{remark}
            Having defined relative entropy, and conditional relative entropy for general distributions, we can immediately use them to define mutual information and conditional mutual information. 
        \end{remark}

        \begin{remark}
            All the properties that we proved for the relative entropy of discrete or continuous random variables are also valid for the general setting considered here. These properties include the nonnegativity, convexity, and the DPI. 
        \end{remark}
        
        The main summary of this section is that we can define the relative entropy can be defined for distributions on very general observation spaces (such as on manifolds, or on function spaces). In particular, the differential entropy of a random variable  $X$ can be recovered as the relative entropy of $P_X$ and an appropriate invariant measure, such as the Lebesgue measure for $\X = \reals^d$. 

        \subsection{Variational Definition I: Gelfand-Yaglom-Peres} 
        \label{subsec:var-def-I-gelfand} 
            In this section, and the next, we present two \emph{variational definitions} of relative entropy for general probability distributions --- that is, we define relative entropy as the solution of an optimization problem. There are several benefits of such a representation: 
            \begin{itemize}
                \item several properties, such as convexity and lower semi-continuity, can be easily inferred, 
                \item such representations easily allow us to get upper or lower bounds by considering specific values of the objective function being optimized. 
            \end{itemize}

            \begin{example}
                \label{example:l1-norm}
                 As a simple example, consider the $\ell_1$ norm of a vector: $\|\xvec\|_1 = \sum_{i=1}^d |x_i|$. It is easy to check that it can also be defined as follows: 
                \begin{align}
                    \|\xvec\|_1 = \sup_{y \in \reals^d: \|y\|_\infty \leq 1} \sum_{i=1}^d x_i y_i. 
                \end{align}
                The above definition immediately implies that the map $\xvec \mapsto \|\xvec\|_1$ is convex: since it is the supremum of linear functions. Furthermore, we also immediately observe that it is lower semi-continuous: since it is the supremum of continuous functions. Furthermore, by choosing any specific value of $\yvec$, we get a lower bound on the $\ell_1$ norm of $\xvec$.             
            \end{example}

            %%%%%%%%
            We now present the first variational definition of relative entropy, which says that for general probability spaces, the relative entropy between two distributions is equal to the supremum of the relative entropy between quantized versions of the two distributions. In other words, for most purposes, analyzing the properties of relative entropy (and derived quantities, such as mutual information) for discrete distributions with finite support is without loss of generality. 
        
            \begin{theorem}
                \label{thm:variational-rep-1}
                Let $(\mc{X}, \mc{F})$ denote a measurable space, and let $P$ and $Q$ denote two probability measures on this space. Let $\mc{E} = \{E_1, E_2, \ldots, E_n\}$ denote a finite disjoint partition of $\mc{X}$, consisting of elements of $\mc{F}$. Then, we have  
                \begin{align}
                    \dkl(P, Q) = \sup_{\mc{E}} \sum_{E \in \mc{E}} P(E) \log \lp \frac{P(E)}{Q(E)} \rp = \sup_{\mc{E}}  \dkl \lp P_{\mc{E}} \parallel Q_{\mc{E}} \rp, 
                \end{align}
                where we have used $P_{\mc{E}}$ and $Q_{\mc{E}}$ to denote the quantized versions of $P$ and $Q$ over the partition $\mc{E}$. 
            \end{theorem}

            \begin{proof}
                We will proof the equality by showing that both $\leq$ and $\geq$ simultaneously hold. The lower bound is an easy consequence of the DPI for relative entropy. In particular, let $\mc{E}$ denote a partition of $\X$ with $m$ elements $\{E_1, \ldots, E_m\}$. Let $f:\X \to [m]$ be a function defined as $f(x) = \sum_{i=1}^m i \boldsymbol{1}_{x \in E_i}$, and let $Y = f(X)$. Then, we have 
                \begin{align}
                    \dkl(P \parallel Q) \stackrel{(i)}{\geq} \dkl\lp P_Y \parallel Q_Y \rp = \dkl \lp P_{X, \mc{E}} \parallel Q_{X, \mc{E}} \rp, 
                \end{align}
                where $(i)$ follows from the DPI for relative entropy. Since $\mc{E}$ above was arbitrary, we can take a supremum over all such finite partitions to get the lower bound. 

                Proving the other direction is more involved, and we proceed in the following steps: 

                \paragraph{Step 1:} We begin by noting that without loss of generality, we can assume $P \ll Q$. Because, if $P \not \ll Q$, then both sides of the required equality are infinite. In particular, if $P \not \ll Q$, then there exists a measurable set $E$, such that $Q(E)=0$ but $P(E)>0$. Then, define $\mc{E} = \{E, E^c\}$, and observe that $\dkl( P_{\mc{E}} \parallel Q_{\mc{E}}) = \infty$. 

                \paragraph{Step 2:} Since $P\ll Q$, there exists a measurable Radon-Nikodym derivative $X = dP/dQ$.  Since $\dkl(P \parallel Q) = \mathbb{E}_Q[\varphi(X)]$, for real valued sequence $c_n \to \infty$, we have $\dkl(P\parallel Q) = \lim_{c \to \infty} \mathbb{E}_Q[\varphi(X) \boldsymbol{1}_{X\leq c}]$ by the monotone convergence theorem~(MCT). 

                \paragraph{Step 3:} Fix a $c>0$ and $\epsilon >0$, and let the integer $n$ denote $c/\epsilon$. Construct a partition $\mc{E} = \{E_0, \ldots, E_n\}$, where the sets $E_0$ through $E_n$ are defined as  
                \begin{align}
                    &E_j = \{\omega: j\epsilon \leq X(\omega) \leq (j+1)\epsilon \}, \text{ for } j=0, \ldots, n-1, \\
                    \text{and} \quad 
                    &E_n = \{\omega: X(\omega) \geq c\}. 
                \end{align}
                Using this partition, define a discrete approximation of $X$, as 
                \begin{align}
                    Y_n = \sum_{j=0}^{n-1} j \epsilon \boldsymbol{1}_{E_j}. 
                \end{align}
                Let $X_c = X \boldsymbol{1}_{X\leq c}$, and note that $Y_n \leq X_c$ and $|Y_n - X_c| \leq \epsilon$. 
                Now, note that for a fixed $c<\infty$, the function $\varphi$ restricted to the domain $[0,c]$ is uniformly continuous. Hence, there exists a $\delta \equiv \delta(c, \epsilon)$, such that we have $|\varphi(Y_n) - \varphi(X_c)|\leq \delta$ almost surely. Furthermore, for a fixed $c$, the term $\delta$ goes to $0$ as $\epsilon \to 0$. 

                The above uniform continuity result implies that 
                \begin{align}
                    \mathbb{E}_Q[\varphi(X_c)] - \delta \leq \mathbb{E}_Q[\varphi(Y_n)] = \sum_{j=0}^{n-1} Q(E_j) \varphi(j\epsilon).  \label{eq:variational-proof-1-1}
                \end{align}
                Now, we observe that 
                \begin{align}
                    \mathbb{E}_Q[j\epsilon] \leq \mathbb{E}_Q[X \boldsymbol{1}_{E_j}] = P(E_j) \leq \mathbb{E}_Q[ (j+1)\epsilon], 
                \end{align}
                which implies that 
                \begin{align}
                    j \epsilon  \leq \frac{P(E_j)}{Q(E_j)} \leq (j+1)\epsilon  \quad \text{or} \quad 
                    \left \lvert \varphi\lp \frac{P(E_j)}{Q(E_j)} \rp - \varphi(j\epsilon) \right \rvert \leq \delta. 
                \end{align}
                Plugging this back into~\eqref{eq:variational-proof-1-1}, we get 
                \begin{align}
                     \mathbb{E}_Q[\varphi(X_c)] - \delta \leq \mathbb{E}_Q[\varphi(Y_n)] = \sum_{j=0}^{n-1} Q(E_j) \varphi\lp \frac{P(E_j)}{Q(E_j)}\rp + \delta = \dkl \lp P_{\mc{E}} \parallel Q_{\mc{E}} \rp + \delta.                    
                \end{align}
                To complete the proof, we take $\epsilon \to 0$ for a fixed $c$, and then take $c \to \infty$. 
            \end{proof}

            We record an immediate consequence of the above result. 
            \begin{corollary}
                % The following two conclusions are immediate from the above result: 
                % \begin{align}
                %     &\dkl(P, Q) \geq \dkl(P_{\mc{E}}, Q_{\mc{E}}), \quad \text{for any } \mc{E}.
                % \end{align}
                For any $\epsilon >0$, there exists a finite partition of $\X$, such that 
                \begin{align}
                    &\dkl(P, Q) - \epsilon \leq \dkl(P_{\mc{E}}, Q_{\mc{E}}). 
                \end{align}
            \end{corollary}
            
        \subsection{Variational Definition II: Donsker-Varadhan} 

            We now present a functional variational representation of relative entropy, due to Donsker and Varadhan. 
            \begin{theorem}
                \label{thm:donsker-varadhan}
                Consider a measurable space $(\X, \mc{F})$, with two probability measures $P$ and $Q$. Suppose $P \ll Q$, and let $\mc{C}_Q$ denote the set of measurable functions $f: \X \to \reals$, such that $\mathbb{E}_Q[e^f(X)]< \infty$. Then, we have 
                \begin{align}
                    \dkl(P\parallel Q) = \sup_{f \in \mc{C}_Q} \mathbb{E}_P[f(X)] - \log \lp  \mathbb{E}_Q[e^{f(X)}]\rp. 
                \end{align}
            \end{theorem}

            \begin{proof}

                First, note that we can restrict our attention to $P\ll Q$. For otherwise, if $P \not \ll Q$, then there must exist an $E$ such that $Q(E)=0$, but $P(E)>0$. Then, define a function $f_c = c \boldsymbol{1}_E$, and note that it lies in $\mc{C}_Q$ for all values of $c \in \reals$. Then, we have 
                \begin{align}
                    \sup_{f \in \mc{C}_Q} \mathbb{E}_P[f(X)] - \log E_Q[e^{f(X)}] \geq \sup_{c > 0}  \mathbb{E}_P[f_c(X)] - \log E_Q[e^{f_c(X)}] = \sup_{c >0} c P(E) = \infty.
                \end{align}

                Hence, we can assume that $P \ll Q$ for the rest of the proof. 
                As in the previous case, we prove the result in two steps to show that both the $\leq$ and $\geq$ hold.
                For any $f \in \mc{C}_Q$, define the \emph{tilted} distribution $Q^f$, such that for any measurable $E$, we have 
                \begin{align}
                    Q^f(E) = \mathbb{E}_Q\lb e^{f(X)-Z_f} \boldsymbol{1}_E(X)\rb, \quad \text{where} \quad 
                    Z_f = \log\lp \mathbb{E}_Q\lb e^{f(X)} \rb\rp. 
                \end{align}
                Now, observe that $Q^f \ll Q$, and 
                \begin{align}
                    \log \lp \frac{dQ^f}{dQ} \rp = f(X) - Z_f, 
                \end{align}
                which implies that 
                \begin{align}
                 \mathbb{E}_P\lb \log \lp \frac{dQ^f}{dQ} \rp    \rb &= \mathbb{E}_P[f(X)] - Z_f. 
                \end{align}
                Next, we note that we also have $Q \ll Q^f$. To see why this is true, suppose there exists an $E$ such that $Q^f(E)=0$, but $Q(E) >0$. Define $A_0 = \{ e^{f(X)-Z_f} \geq 1\} \cap E$, and for $i \geq 1$, define $A_i = \{ e^{f(X) - Z_f} \in (1/(i+1), 1/i]\} \cap E$. Then, we have $A_i \cap A_j = \emptyset$ for $i \neq j$, and furthermore, $E = \cup_{i=0}^{\infty} A_i$. Due to the countable additivity, we have $Q(E) = \sum_{i=0}^{\infty} Q(A_i)$, which means that there must exist an $n$, such that $Q(A_n)>0$. This implies that
                \begin{align}
                    0 = Q^f(E) = \sum_{i=0}^{\infty} Q^f(A_i) \geq \sum_{i=0}^{\infty} \frac{Q(A_i)}{i+1} \geq \frac{Q(A_n)}{n+1} > 0, 
                \end{align}
                which is a contradiction. Hence, we have proved that $Q \ll Q^f$, which in turn implies that $P \ll Q^f$, and hence $dP/dQ^f$ is well defined

                
                Now, let $G$ denote the set $\{ dP/dQ > 0\}$ and note that $\mathbb{E}_P[\log(dQ^f/dQ)] = \mathbb{E}_P[\log(dQ^f/dQ) \boldsymbol{1}_G]$. Introduce the probability measures $\Qtilde$ and $\Qtilde^f$, such that $\Qtilde(E) = Q(E \cap G)$, and $\Qtilde^f(E) = \Qtilde(E \cap G)$. Then, we can write 
                \begin{align}
                    \mathbb{E}_P \lb \log \lp \frac{dQ^f}{dQ}\rp  \rb &= \mathbb{E}_P \lb \log \lp \frac{dQ^f}{dQ}\rp \boldsymbol{1}_{G}  \rb =  \mathbb{E}_P \lb \log \lp \frac{d\Qtilde^f}{dP} \frac{dP}{d\Qtilde} \rp \boldsymbol{1}_{G}  \rb  \\
                    & =   \mathbb{E}_P \lb \log \lp  \frac{dP}{d\Qtilde} \rp \boldsymbol{1}_{G}  \rb  - \mathbb{E}_P \lb \log \lp  \frac{dP}{d\Qtilde^f} \rp \boldsymbol{1}_{G}  \rb  \\
                     & =   \mathbb{E}_P \lb \log \lp  \frac{dP}{dQ} \rp   \rb  - \mathbb{E}_P \lb \log \lp  \frac{dP}{dQ^f} \rp   \rb  \\                   
                     & = \dkl(P \parallel Q) - \dkl(P \parallel Q^f) \leq \dkl(P \parallel Q). 
                \end{align}
                Since $f$ was an arbitrary element of $\mc{C}_Q$, this completes the proof of one side of the inequality. 

                To show the other direction, we will rely on~\Cref{thm:variational-rep-1}. 
                Now consider the case when $P \ll Q$. Then, consider any partition $\mc{E}$ of the domain, and define $f$ as 
                \begin{align}
                    f = \sum_{E \in \mc{E}} \log \lp \frac{P(E)}{Q(E)} \rp \boldsymbol{1}_E. 
                \end{align}
                For this function, the objective function is 
                \begin{align}
                    \mathbb{E}_P[f(X)] - \log \mathbb{E}_Q[e^{f(X)}] &= \sum_{E}P(E) \log \lp \frac{P(E)}{Q(E)} \rp - \log \lp \sum_{E\in E} Q(E) \frac{P(E)}{Q(E)} \rp \\
                    & = \dkl \lp P_{\mc{E}} \parallel Q_{\mc{E}} \rp 
                \end{align}
                For all choices of the partition $\mc{E}$, the corresponding $f$ is still a simple function, and hence, it also belongs to $\mc{C}_Q$. Denoting the class of simple functions by $\mc{S}$, we then obtain 
                \begin{align}
                    \sup_{f \in \mc{C}_Q} \mathbb{E}_P[f(X)] - \log E_Q[e^{f(X)}] \geq 
                    \sup_{f \in \mc{S}} \mathbb{E}_P[f(X)] - \log E_Q[e^{f(X)}]  \geq \sup_{\mc{E}} \dkl(P_{\mc{E}}, Q_{\mc{E}}). 
                \end{align}
                The last term is precisely the definition of $\dkl(P\parallel Q)$ from~\Cref{thm:variational-rep-1}. 
            \end{proof}

            \begin{remark}
                \label{remark:DV-approximation-1} A simple consequence of the above result is that if $\dkl(P \parallel Q)$ is small, then we should expect $\mathbb{E}_P[f(X)]$ to be well approximated by $\mathbb{E}_Q[f(X)]$. To see why, note that $e^{x} \approx 1 + x$ and $\log(1 + x) \approx x$ for small enough $x$. Thus choosing any function $f \in \mc{C}_Q$, we have 
                \begin{align}
                    \dkl(P \parallel Q) \geq \mathbb{E}_P[f(X)] - \log \lp \mathbb{E}_Q[e^{f(X)}] \rp \approx \mathbb{E}_P[f(X)] - \log \lp 1 +  \mathbb{E}_Q[f(X)] \rp  \approx \mathbb{E}_P[f(X)] - \mathbb{E}_Q[f(X)]. 
                \end{align}
                Thus, a small value of $\dkl(P \parallel Q)$ implies that the expected values of $f(X)$ under $P$ and $Q$ must be close to each other. 
            \end{remark}
            
            Since mutual information between $(X,Y)$ is the relative entropy between the joint distribution and the product of marginals, we have the following corollary. 
            \begin{corollary}
                \label{corollary:mi-donsker-varadhan} 
                For $(X, Y) \sim P_{XY}$ on $\X \times \Y$, let $\mc{C}$ denote the class of functions $\{f: \X \times \Y \to \reals: e^f \in L^1(P_X \times P_Y) \}$. Then, we have 
                \begin{align}
                    I(X;Y) = \sup_{f \in \mc{C}} \mathbb{E}_{P_{XY}}[f(X, Y)] - \log \mathbb{E}_{P_X \times P_Y} \lb e^{f(X, Y)} \rb. 
                \end{align}
            \end{corollary}

            \paragraph{\red{Application:} Generalization bound in machine learning.}  In statistical learning theory, we are usually given a training dataset $S = (Z_1, \ldots, Z_n) \in \mc{Z}^n$ consisting on $n$ \iid training points drawn from a distribution $P$. A learning algorithm, $\mc{A}$, is a channel (or a Markov kernel) from $\mc{Z}^n$ to a `hypothesis class' $\mc{H}$. Given a loss function $\ell:\mc{H} \times \mc{Z} \to \reals$, we can define the population and empirical risk of a hypothesis $h \in \mc{H}$ as 
            \begin{align}
                L_P(h) = \mathbb{E}_{Z \sim P}[\ell(h, Z)],  \quad \text{and} \quad 
                L_S(h) = \frac{1}{n} \sum_{Z_i \in S} \ell(h, Z_i). 
            \end{align}
            Now, let $H$ denote the possibly random output of a learning algorithm $\mc{A}$ on a training set $S$. Then, the generalization error of this algorithm is the expected difference between the test and training risk of $H$:
            \begin{align}
                \text{gen}(P, P_{H|S}) = \mathbb{E}\lb L_P(H) - L_S(H) \rb = \mathbb{E}\lb L_{S'}(H) - L_S(H) \rb, 
            \end{align}
            where $S'$ is an independent copy of $S$. Now, we can state the following bound over the generalization error. 
            \begin{proposition}[Xu and Raginsky, 2016]
                \label{prop:generalization-error} 
                Suppose the loss function is such that $\ell(h, Z)$ is $\sigma^2$-subGaussian for all $h \in \mc{H}$. Then, we have 
                \begin{align}
                    \text{gen}(P, P_{H|S}) \leq \sqrt{ \frac{2 \sigma^2 I(S; H)}{n}}. 
                \end{align}
            \end{proposition}
            \begin{proof}
                To prove this result, we note the following: 
                \begin{align}
                    I(S; H) = \sup_{f} \mathbb{E}_{P_{SH}}[f(S, H)] - \log \mathbb{E}_{P_S \times P_H}\lb e^{f(S, H)} \rb. \label{eq:generalization-1}
                \end{align}
                To get the bound, we will select a specific $f(s, h) = \frac{1}{n} \sum_{z_i \in s} \ell(h, z_i)$, and note that $f$ is $(\sigma^2/n)$-subGaussian. Furthermore, note that the generalization error is equal to 
                \begin{align}
                    \text{gen}(P, P_{H|S}) = \mathbb{E}_{P_{SH}}\lb f(S, H) \rb - \mathbb{E}_{P_{S} \times P_{H}}[f(S', H')]. 
                \end{align}
                For any $\lambda \in \reals$, plugging  $\lambda f$ in~\eqref{eq:generalization-1}, we get 
                \begin{align}
                    I(S;H) &\geq \mathbb{E}_{P_{SH}}\lb \lambda f(S, H) \rb - \log \lp \mathbb{E}_{P_S \times P_H}\lb e^{\lambda f(S',H')} \rb  \rp \\
                    & \geq \mathbb{E}_{P_{SH}}\lb \lambda f(S, H) \rb - \log \lp \exp \lp \frac{\lambda^2 \sigma^2}{2n} + \lambda \mathbb{E}_{P_S \times P_H}[f(S', H') \rp  \rp \\
                    & = \mathbb{E}_{P_{SH}}[\lambda f(S, H)] - \mathbb{E}_{P_{S}\times P_H}[\lambda f(S', H')] - \frac{\lambda^2 \sigma^2}{2n} \\
                    & = \lambda \, \text{gen}(P, P_{H|S}) - \frac{\lambda^2 \sigma^2}{2n}. 
                \end{align}
                On optimizing for $\lambda$, we get 
                \begin{align}
                    I(S; H) \geq \sup_{\lambda} \lambda \, \text{gen}(P, P_{H|S}) - \frac{\lambda^2 \sigma^2}{2n}  = \frac{n \, \text{gen}^2(P, P_{H|S})}{2 \sigma^2}. 
                \end{align}
                On rearranging, we get the required 
                \begin{align}
                    \text{gen}(P, P_{H|S}) \leq \sqrt{ \frac{ 2 \sigma^2 I(S; H)}{n}}. 
                \end{align}
            \end{proof}
            Now, suppose $h^*$ denotes the element of $\mc{H}$ that achieves the minimum expected loss. How does $\mathbb{E}[L_P(H)]$ compare with $L_P(h^*)$? To analyze this, note that
            \begin{align}
                \mathbb{E}[L_P(H)] \leq \text{gen}(P, P_{H|S}) + \mathbb{E}_{P_{SH}} \lb L_S(H) \rb. 
            \end{align}
            Hence, on subtracting $L_S(h^*)$ on both sides, and using~\Cref{prop:generalization-error}, we get 
            \begin{align}
                \mathbb{E}[L_P(H)] - L_P(h^*) \leq \sqrt{\frac{2 \sigma^2 I(S; H)}{n}} + \mathbb{E}_{P_{SH}} \lb L_S(H) - L_S(h^*)\rb. 
            \end{align}
            Consider the following two cases: 
            \begin{itemize}
                \item If $|\mc{H}|<\infty$, then we can upper bound $I(S; H)$ with $H(H) \leq \log(|\mc{H}|)$. 
                \item More generally, suppose we consider an appropriate $\epsilon$-covering set of $\mc{H}$, denoted by $|\mc{H}_{\epsilon}|$. Then, we can bound $I(S; H)$ with $\log(|\mc{H}_{\epsilon}|)$, which recovers the metric entropy bound. 
            \end{itemize}
            Note that if $H$ is selected by an empirical risk minimization~(ERM) method, the second term on the RHS is upper bounded by $0$. 
            
            
            


            
    \section{$f$-divergences}
        \begin{definition}
            \label{def:f-divergence} Suppose $f:(0, \infty) \to \reals$ is a convex function with $f(1)=0$, and $f(0) \defined \lim_{x\to 0} f(x)$. Then, the $f$-divergence between two distributions $P$ and $Q$, with $P\ll Q$ is defined as 
            \begin{align}
                D_f(P\parallel Q) \defined 
                    \mathbb{E}_{Q} \lb f\lp \frac{dP}{dQ}(X) \rp \rb, \quad \text{if } P \ll Q. 
            \end{align}
            For general $P$ and $Q$, let $\mu$ denote a common dominating measure~(such as $P+Q$), and $q = dQ/d\mu$ and $p = dP/d\mu$. Then, the $f$-divergence between $P$ and $Q$ is defined as 
            \begin{align}
                D_f(P \parallel Q) = \int_{q>0} q(x) f\lp \frac{p(x)}{q(x)} \rp d\mu + \mathbb{P}\lp q=0\rp \lim_{x \to \infty} x f(1/x).
            \end{align}
            If $\mathbb{P}(q=0)=0$, then the second term is assumed to be $0$, irrespective of $\lim_{x \to \infty}xf(1/x)$. 
        \end{definition}

        \begin{remark}
            \label{remark:discrete-f-div} For discrete distributions, we have the following simple definition of $f$-divergences: 
            \begin{align}
                D_f(P \parallel Q) = \sum_{x \in \X} Q(x) f\lp \frac{P(x)}{Q(x)} \rp. \label{eq:discrete-f-div}
            \end{align}
        \end{remark}

        \begin{remark}
            \label{remark:instances-of-f-div} 
            The above definition unifies several popular statistical divergences, such as 
            \begin{itemize}
                \item Relative entropy, with $f(x) = x \log x$ 
                \item Total variation, with $f(x) = \frac{1}{2}|x-1|$ 
                \item Squared Hellinger distance, with $f(x) = (1-\sqrt{x})^2$
                \item $\chi^2$-distance, with $f(x) = (x-1)^2$. 
            \end{itemize}
        \end{remark}
        We can develop several properties of $f$-divergences, that parallel those for relative entropy: see~\citet[Chapter 7]{polyanskiy2023ITbook} for a thorough discussion. Below, we state and prove the nonnegativity, convexity, and DPI for $f$-divergences. 
        % 
        \begin{proposition}
            \label{prop:f-div-properties} For any convex $f:[0, \infty]$, with $f(1) = 0$, and $f(0) \defined \lim_{x \downarrow 0} f(x)$, we have 
            \begin{enumerate}[label=(\alph*)]
                \item $D_f(P\parallel Q) \geq 0$, for all $P, Q$. Furthermore, if $f$ is strictly convex, then the equality holds only for $P=Q$. 
                \item $D_f(P_{XY} \parallel Q_{XY} ) \geq D_f(P_X \parallel Q_X)$. 
                \item The mapping $(P, Q) \mapsto D_f(P \parallel Q)$ is convex.  
                \item Suppose $P_{XY} = P_X \times P_{Y|X}$ and $Q_{XY} = Q_X \times P_{Y|X}$. Then, we have the following data-processing inequality:
                \begin{align}
                    D_f(P_Y \parallel Q_Y) \leq D_f(P_X \parallel Q_X). 
                \end{align}
           \end{enumerate}
        \end{proposition}
        % 
        \begin{proof}
            We will prove the statements under the simplifying assumption that $P_{XY} \ll Q_{XY}$. 
            \begin{enumerate}[label=(\alph*)]
                \item The result follows from a direct application of Jensen's inequality. In particular, we have 
                \begin{align}
                    D_f(P \parallel Q) = \mathbb{E}_Q\lb f\lp \frac{dP}{dQ}(X) \rp \rb \geq f\lp \mathbb{E}_Q\lb \frac{dP}{dQ}(X) \rb \rp = f(1) = 0. 
                \end{align}
                If $f$ is not affine, then the equality holds only if $dP/dQ$ is a constant $Q$-almost-surely. 
                \item For simplicity, we will prove this monotonicity result for the simple case of discrete distributions. 
                \begin{align}
                    D_f(P_{XY} \parallel Q_{XY}) &= \sum_{x, y} q(x, y) f\lp \frac{p(x,y)}{q(x,y)} \rp  = \sum_{x \in \X} q(x) \sum_{y \in \Y} q(y|x) f\lp \frac{p(x) p(y|x)}{q(x) q(y|x)} \rp  \\
                    & \geq \sum_{x \in \X} q(x) f \lp \sum_{y \in \Y} q(y|x) \frac{p(x) p(y|x)}{q(x) q(y|x)} \rp  \\ 
                    & \geq \sum_{x \in \X} q(x) f \lp \frac{p(x)}{q(x)} \sum_{y \in \Y} p(y|x) \rp  = \sum_{x \in \X} q(x) f\lp \frac{p(x)}{q(x)} \rp = D_f(P_X \parallel Q_X). 
                \end{align}
                \item The proof of this result uses the monotonicity. In particular, let $P_0, P_1$ and $Q_0, Q_1$ denote two pairs of distributions. Now, let $X \sim \text{Bernoulli}(\lambda)$ for some $\lambda \in [0,1]$. Then, define the following joint distributions ($P_{XY}$ and $Q_{XY}$): 
                \begin{align}
                    P_{Y|X=0} = P_0, \quad P_{Y|X=1} = P_1, \quad \text{and} \quad Q_{Y|X=0} = Q_0, \quad Q_{Y|X=1} = Q_1. 
                \end{align}
                Then, by the monotonicity result, we have 
                \begin{align}
                    D_f(P_{XY} \parallel Q_{XY}) &= \barlambda \sum_{y} Q_0(y) f \lp \frac{\barlambda P_0(y)}{\barlambda P_1(y)} \rp  + \lambda \sum_{y} Q_1(y) f\lp \frac{\lambda P_1(y)}{\lambda Q_1(y)} \rp \\
                    & \geq D_f(P_Y \parallel Q_Y) \\
                    & = D_f \lp \barlambda P_0 + \lambda P_1 \parallel \barlambda Q_0 + \lambda Q_1 \rp. 
                \end{align}
                \item Again, we prove this in the simple setting of discrete distribution. By monotonicity, we know that 
                \begin{align}
                    D_f(P_{Y} \parallel Q_Y) \leq D_f(P_{XY} \parallel Q_{XY}). 
                \end{align}
                Note that since the conditional distributions $P_{Y|X}$ and $Q_{Y|X}$ are the same~(i.e., the marginals $P_X$ and $P_Y$ are passed through the same channel), we have 
                \begin{align}
                    D_f(P_{XY} \parallel Q_{XY}) &= \sum_{x \in \X}q(x) \sum_{y \in \Y} q(y|x) f \lp \frac{p(x) p(y|x)}{q(x)q(y|x)} \rp  \\
                    &= \sum_{x \in \X}q(x)  \sum_{y \in \Y} q(y|x)  f \lp \frac{p(x)}{q(x)} \rp \\
                    &= \sum_{x \in \X}q(x) f \lp \frac{p(x)}{q(x)} \rp \sum_{y \in \Y} q(y|x)   = D_f(P_X \parallel Q_X). 
                \end{align}
            \end{enumerate}
        \end{proof}
        
        
        As for the case of relative entropy, we can obtain the analogs of the two variational definitions for $f$-divergences as well. 

        \begin{theorem}
            \label{thm:f-div-var-def-1}   Let $P$ and $Q$ be any two probability measures defined on some measurable space $(\X, \mc{F})$. For any finite disjoint partition of $\X$, denoted by $\mc{E} = \{E_i \in \mc{F}: 1 \leq i \leq n\}$, let $P_{\mc{E}}$ and $Q_{\mc{E}}$ denote the discrete distributions with pmfs $\{P(E): E \in \mc{E}\}$ and $\{Q(E): E \in \mc{E}\}$. Then, we have the following: 
            \begin{align}
                D_f(P \parallel Q) = \sup_{\mc{E}} D_f(P_{\mc{E}} \parallel Q_{\mc{E}})  = \sup_{\mc{E}} \sum_{E \in \mc{E}} Q(E) f\lp \frac{P(E)}{Q(E)} \rp. 
            \end{align}
        \end{theorem}
        The proof of this statement follows the general argument used in proving~\Cref{thm:variational-rep-1} for the analogous result for relative entropy. We refer the reader to~\citet[\S~7.14]{polyanskiy2023ITbook} for details. 
        
        
        Next, we state and prove an analog of~\Cref{thm:donsker-varadhan} for $f$-divergences on $\X = \mathbb{R}^d$.  
        \begin{theorem}
            Let $P \ll Q$ be two distributions on $\X = \mathbb{R}^d$, and furthermore assume that both $P$ and $Q$ admit densities ($p$, and $q$ respectively) w.r.t. the Lebesgue measure $\mu$. With $f^*$ denoting the convex-conjugate of $f$, we have the following variational representation of $D_f(P\parallel Q)$: 
            \begin{align}
                D_f(P\parallel Q) = \mathbb{E}_Q \lb f\lp \frac{dP}{dQ}(X) \rp \rb = \sup_{g:\X \to \reals} \mathbb{E}_P[g(X)] - \mathbb{E}_Q\lb f^*(g(X)) \rb, 
            \end{align}
            where $g$ is restricted to ensure that both expectations are finite. 
        \end{theorem}
        \begin{proof}
            The proof follows directly from the definition of convex conjugates. First note that since $f$ is convex, we have $f = (f^*)^*$. Hence, for any $u \in (0, \infty)$, we have 
            \begin{align}
                f(u) = \sup_{v \in \reals} u v - f^*(v). 
            \end{align}
            Thus, with $u= p(x)/q(x)$, we have 
            \begin{align}
                f\lp \frac{p(x)}{q(x)} \rp = \sup_{v \in \reals} \frac{p(x) v}{q(x)} - f^*(v). 
            \end{align}
            For any function $g$, plugging the value of $v = g(x)$ above gives us a lower bound on $f(p(x)/q(x))$. Hence, for an arbitrary function $g$, we have 
            \begin{align}
                D_f(P \parallel Q) \geq \sup_{g } \int_{\X} q(x) \lp g(x) \frac{p(x)}{q(x)} - f^*(g(x)) \rp dx = \sup_{g} \mathbb{E}_P[g(X)]  - \mathbb{E}_Q[f^*(g(X))]. 
            \end{align}
            To show the other direction: fix an arbitrary $\epsilon >0$, and define $g_\epsilon$ as the function with values $g(x) = v_x$; where $v_x$ ensures $v_x p(x)/q(x)- f^*(v_x) \geq f(p(x)/q(x)) - \epsilon$. Plugging this in, we get 
            \begin{align}
                D_f(P \parallel Q)  \leq   \mathbb{E}_P[g_\epsilon(X)] - \mathbb{E}_Q[f^*(g_\epsilon(X))]  + \epsilon. 
            \end{align}
        \end{proof}

        \begin{example}[$\chi^2$-divergence]
            \label{example:chi-squared-variational-def} Recall that $\chi^2$ distance corresponds to $f(x) = (x-1)^2$. The convex conjugate of this function is equal to $f^*(u) = u^2/4 + u =  (u/2 + 1)^2 - 1$. To see this, note that 
            \begin{align}
                f^*(u) = \sup_{v \in \reals} uv - (v-1)^2 = sup_{v \in \reals} g(v)
            \end{align}
            Note that $g$ is a concave function, and $g'(v) = u - 2(v-1)$. Hence, its optimal value is achieved at $v^* = 1 + u/2$, and is equal to 
            \begin{align}
                f^*(u) = u \lp \frac{u}{2} + 1 \rp - \frac{u^2}{4} = \frac{u^2}{4} + u = \lp \frac{u}{2} + 1 \rp^2- 1. 
            \end{align}
            Plugging this into the variational definition of $f$-divergences, we get 
            \begin{align}
                \chi^2(P\parallel Q) &= \sup_{g} \mathbb{E}_P[g(X)] - \mathbb{E}_Q\lb \lp \frac{g(X)}{2} + 1 \rp^2 \rb + 1  \\
                & = \sup_{g} 2 \mathbb{E}_P \lb \lp \frac{g(X)}{2} +1 \rp \rb - \mathbb{E}_Q \lb \lp \frac{g(X)}{2} + 1 \rp^2 \rb - 1 \\
                & = \sup_{h} 2 \mathbb{E}_P[h(X)] - \mathbb{E}_Q[h^2(X)] - 1, 
            \end{align}
            where in the last inequality, we used the change of variable $h(x) \leftarrow g(x)/2 + 1$. 

            Note that the above variational definition is not scale-invariant, and we can replace $h(\cdot)$ with an arbitrary $\lambda h(\cdot)$, to get 
            \begin{align}
                \chi^2(P \parallel Q) = \sup_{h, \lambda} 2 \lambda \mathbb{E}_P[h(X)] - \lambda^2 \mathbb{E}_Q[h^2(X)] - 1. 
            \end{align}
            On optimizing over $\lambda$, we get the following, more interpretable, definition 
            \begin{align}
                \chi^2(P \parallel Q) = \sup_{h } \frac{\lp \mathbb{E}_P[h(X)] - \mathbb{E}_Q[h(X)] \rp^2}{\var_Q\lp h(X) \rp }. 
            \end{align}
            That is, two distributions $P$ and $Q$ are distinguishable in $\chi^2$-divergence, if there exists a function $h$, for which the the absolute difference in means under $P$ and $Q$ is much larger than the standard deviation of $h(X)$ under $Q$. 
        \end{example}

        \paragraph{\red{Application}: Estimation error bounds.} Let us consider an estimation problem in the simplest case, where the parameter set $\Theta$ is a subset of $\reals$. Now, let $\theta$ denote the true parameter, $X \sim P_{\theta}$, and $\thetahat = \thetahat(X)$ is some estimate of $\theta$ based on $X$. That is, we have the Markov chain $\theta \rightarrow X \rightarrow \thetahat$. 
        \begin{proposition}[Hammersley-Chapman-Robbins]
        \label{prop:hammersley}
             In the setting described above, the expected squared error of any estimator $\thetahat$ satisfies 
            \begin{align}
                \mathbb{E}_{\theta} [(\theta - \thetahat)^2] \geq \var_{\theta}(\thetahat) \geq \sup_{\theta' \neq \theta} \frac{ \lp \mathbb{E}_{\theta}[\thetahat(X)] -  \mathbb{E}_{\theta'}[\thetahat(X)]\rp^2}{\chi^2\lp P_{\theta'} \parallel P_{\theta} \rp}.  
            \end{align}           
        \end{proposition}
        % 
        \begin{proof}
            This result follows by a simple application of the DPI for $f$-divergences, and the variational definition of $\chi^2$-divergence obtained in~\Cref{example:chi-squared-variational-def}. In particular, let $\theta' \in \Theta$ such that $\theta' \neq \theta$. Then, with $P_X = P_{\theta'}$ and $Q_X=P_{\theta}$, we have
            \begin{align}
             \chi^2\lp P_X \parallel Q_X  \rp \stackrel{(i)}{\geq}   \chi^2\lp P_{\thetahat} \parallel Q_{\thetahat}  \rp \stackrel{(ii)}{\geq} \frac{ \lp \mathbb{E}_{\theta}[\thetahat] - \mathbb{E}_{\theta'}[\thetahat] \rp^2}{\var_{\theta}(\thetahat)},  
            \end{align}
            where $(i)$ is due to the data-processing inequality, and $(ii)$ is due to the variational representation of $\chi^2$-divergence with $h(x) = x$.  On rearranging, we get 
            \begin{align}
                \var_{\theta}(\thetahat) \geq \frac{ \lp \mathbb{E}_{\theta}[\thetahat] - \mathbb{E}_{\theta'}[\thetahat] \rp^2}{\chi^2(P_{\theta'} \parallel P_{\theta})}.   
            \end{align}
            As $\theta' \neq \theta$ is an arbitrary element of $\Theta$, the result follows by taking a supremum over all such $\theta'$. 
        \end{proof}
        % 
        \begin{corollary}[Cramer-Rao lower bound]
            \label{corollary:cramer-rao} Suppose that the distributions $\{P_{\theta}: \theta \in \Theta \subset \reals\}$ admit densities (w.r.t. Lebesgue measure) that are twice continuously differentiable. If we restrict our attention to unbiased $\thetahat$, then~\Cref{prop:hammersley} implies the following: 
            \begin{align}
                \mathbb{E}_{\theta}[(\thetahat-\theta)^2] = \var_{\theta}(\thetahat) \geq \frac{1}{J_F(\theta)}, \quad \text{where} \quad J_F(\theta) = \mathbb{E}_{\theta}\lb \lp \frac{\partial \log(p_{\theta}(X))}{\partial \theta} \rp^2\rb. 
            \end{align}
        \end{corollary}
        \begin{proof}
            The conclusion follows from a fact about $\chi^2$-distance that 
            \begin{align}
                \chi^2(P_{\theta'} \parallel P_{\theta}) = J_F(\theta) (\theta - \theta')^2 + o\lp (\theta - \theta')^2 \rp. 
            \end{align}
            To see why this is true, consider the definition of $\chi^2$-divergence between $P_{\theta'}$ and $P_{\theta'}$ 
            \begin{align}
                \chi^2(P_{\theta'} \parallel P_{\theta}) = \int_{\reals} p_{\theta}(x) \lp \frac{p_{\theta'}(x)}{p_{\theta}(x)} - 1 \rp^2 dx = \int_{\reals} \frac{\lp p_{\theta'}(x) - p_{\theta}(x) \rp^2}{p_{\theta}(x)} dx. 
            \end{align}
            Now, by a Taylor' expansion around $p_{\theta}(x)$, we have 
            \begin{align}
                p_{\theta'}(x) = p_{\theta}(x) + \dot{p}_{\theta}(x) (\theta' - \theta) + o \lp |\theta'-\theta| \rp. 
            \end{align}
            Plugging this approximation back into the definition of $\chi^2(P_{\theta'} \parallel \theta)$, we get that 
            \begin{align}
                \chi^2(P_{\theta'} \parallel P_{\theta}) = (\theta' - \theta)^2 \int \frac{\dot{p}_{\theta}^2(x)}{p_{\theta}(x)} dx   + o\lp (\theta'-\theta)^2 \rp. 
            \end{align}
        \end{proof}

